## 第7章：品質メトリクスとテストカバレッジ


## 🎯 この章の目的

1. **テストと品質を定量的に評価**する指標を理解する
2. **カバレッジ・欠陥密度・安定度**などの実践的メトリクスを導入する
3. **チームが品質を「数値で語れる」状態**を作る

---

## 🧩 1. 品質メトリクスとは何か

📘 「品質メトリクス」とは、

> ソフトウェアの品質を**客観的な数値**で示す指標のこと。

💡 メトリクスを導入する目的：

| 目的     | 内容              |
| ------ | --------------- |
| 状況の可視化 | 現状を数字で把握する      |
| 改善の方向性 | どこを強化すべきかを明確にする |
| 継続的改善  | 時系列変化で品質の傾向を追う  |

例：

> 「テストカバレッジ85％」「欠陥再発率3％」「MTTR12時間」

---

## 🧩 2. 主な品質メトリクス一覧

| カテゴリ       | 指標           | 意味                | 目安値（良好）        |
| ---------- | ------------ | ----------------- | -------------- |
| **テスト網羅性** | ステートメントカバレッジ | 実行された行の割合         | 80%以上          |
|            | ブランチカバレッジ    | 条件分岐を通過した割合       | 70%以上          |
| **欠陥関連**   | 欠陥密度         | LOCあたりのバグ数        | 0.5件/1000LOC以下 |
|            | 再発率          | 修正後に再発したバグの割合     | 5%以下           |
| **安定性**    | MTTR（平均復旧時間） | バグ修正までの平均時間       | 24時間以内         |
| **変更影響**   | 変更失敗率        | デプロイ後にリカバリが必要な率   | 15%以下          |
| **自動化率**   | 自動テスト比率      | 全テストのうち自動化されている割合 | 70%以上          |

💡 メトリクスは「数値そのもの」よりも「傾向」で評価するのが重要です。

---

## 🧩 3. テストカバレッジの測定

📘 **カバレッジ（Coverage）**は、

> 「コードのうちどれだけがテストで実行されたか」を示す割合です。

💻 Pythonでは `coverage.py` が標準的なツールです。

```bash
coverage run -m pytest
coverage report
coverage html
```

💡 出力例（テキスト形式）：

```
Name                        Stmts   Miss  Cover
----------------------------------------------
app/models/reservation.py      45      3    93%
app/usecases/create.py         38      8    79%
----------------------------------------------
TOTAL                          83     11    87%
```

💡 HTMLレポートでは、
未実行行が赤く表示され、視覚的に確認可能です。

---

## 🧩 4. カバレッジの種類

| 種類               | 説明          | 検証対象        |
| ---------------- | ----------- | ----------- |
| **ステートメントカバレッジ** | コード行を実行した割合 | 基本的な網羅性     |
| **ブランチカバレッジ**    | if文などの分岐網羅  | 複雑ロジックの検証   |
| **パスカバレッジ**      | 全経路の組合せ     | テスト設計レベルで活用 |
| **条件カバレッジ**      | 真偽条件ごとの網羅   | 高リスク箇所の監査用  |

💡 推奨：

> 単体テストでは「ブランチカバレッジ」、
> 重要な制御系ロジックでは「パスカバレッジ」も確認する。

---

## 🧩 5. 欠陥密度と再発率

📘 「欠陥密度」は品質の“粗さ”を測る代表指標です。

💡 計算式：

```
欠陥密度 ＝ 発見されたバグ数 ÷ 開発規模（KLOC）
```

💡 例：
10,000行のコードでバグ5件 → 欠陥密度 = 0.5件/KLOC

📘 「再発率」は、修正後の不具合再発を測ります。

```
再発率 ＝ 再発バグ数 ÷ 修正済みバグ総数 × 100%
```

💡 高い再発率は「原因分析不足」や「テストケース漏れ」を示します。

---

## 🧩 6. MTTR（平均復旧時間）

📘 **Mean Time To Recovery** は、

> 「障害が発生してから復旧するまでの平均時間」です。

💡 計算式：

```
MTTR = (復旧完了時刻 - 障害検出時刻) の平均
```

💡 短縮のためのポイント：

* アラートの自動化（監視連携）
* 再現テストの即時実行
* 自動ロールバック対応

💡 DevOps指標としても重要（DORAメトリクスの1つ）。

---

## 🧩 7. 自動化率と品質トレンド

📘 自動化率は、

> 「全テストのうち自動実行できる割合」です。

💡 測定式：

```
自動化率 = 自動テスト件数 ÷ 総テスト件数 × 100%
```

📘 品質トレンドを可視化する例（テキスト表）：

| 期間      | カバレッジ | 欠陥密度 | 自動化率 | コメント   |
| ------- | ----- | ---- | ---- | ------ |
| 2025/06 | 72%   | 0.8  | 65%  | 自動化強化中 |
| 2025/07 | 81%   | 0.6  | 78%  | 安定化傾向  |
| 2025/08 | 88%   | 0.4  | 85%  | 高品質維持  |

💡 「上がっている／下がっている」を数字で議論できることがチームの強みになります。

---

## 🧩 8. DORAメトリクスとCI/CD品質

📘 Googleが提唱した **DORAメトリクス** は、
ソフトウェアデリバリの成熟度を測る指標群です。

| 指標       | 内容              | 目標     |
| -------- | --------------- | ------ |
| デプロイ頻度   | リリースの頻度         | 1日〜週単位 |
| 変更リードタイム | コード変更→本番反映までの時間 | 1日以内   |
| 変更失敗率    | リリースで障害が出る割合    | 15%未満  |
| MTTR     | 障害復旧までの平均時間     | 数時間以内  |

💡 これらを継続的にトラッキングすることで、
品質と開発速度を両立する DevOps の成熟度を定量管理できます。

---

## 🧩 9. メトリクスの活用ルール

📘 メトリクスは「評価」ではなく「改善の指針」として使うことが重要です。

💡 NGな使い方：

* カバレッジを100％に強制する
* 数値だけで人を評価する

💡 OKな使い方：

* 指標を“チームの健康診断”として扱う
* 数値を「次の改善アクション」につなげる

📘 継続的改善サイクル：

```
測定 → 可視化 → 分析 → 改善 → 再測定
```

💡 このループを繰り返すことで、品質は自然に安定していく。

---

## ✅ まとめ

| 要点      | 内容                     |
| ------- | ---------------------- |
| 品質メトリクス | 品質を数値で可視化する仕組み         |
| カバレッジ   | コード網羅率を測る基本指標          |
| 欠陥密度    | バグ発生率を定量評価             |
| MTTR    | 復旧スピードを測る信頼性指標         |
| DORA    | DevOps全体の成熟度を数値化       |
| 成果      | 「品質を語る会話」がデータに基づく文化になる |

---

## 🔜 次章予告：「第8章　AI連携による品質監視と自動分析」

次章では、テストと運用の境界を超えて、
**AIがリアルタイムで品質状態を監視・解析**する仕組みを扱います。
アラート閾値の自動チューニングや、障害原因推定の自動化など、
**運用フェーズのAI活用**に踏み込みます。

